# Entropia de Shannon

A entropia de Shannon é uma medida de incerteza ou quantidade de informação em uma variável aleatória. Proposta por Claude Shannon, um engenheiro elétrico e matemático, em seu trabalho seminal sobre teoria da informação, publicado em 1948.

A entropia de Shannon é definida matematicamente como:

$$
H(X) = -\sum_{i=1}^{n} P(x_i) \log_2 P(x_i)
$$


