# Entropia de Shannon

A entropia de Shannon é uma medida de incerteza ou quantidade de informação em uma variável aleatória. Proposta por Claude Shannon, um engenheiro elétrico e matemático, em seu trabalho seminal sobre teoria da informação, publicado em 1948.

A entropia de Shannon é definida matematicamente como:

$$
H(X) = -\sum_{i=1}^{n} P(x_i) \log_2 P(x_i)
$$


Onde:
- \(H(X)\) é a entropia da variável aleatória \(X\).
- \(n\) é o número de possíveis valores que \(X\) pode assumir.
- \(P(x_i)\) é a probabilidade de \(X\) assumir o valor \(x_i\).

O logaritmo está na base 2, o que implica que a entropia é medida em bits.

